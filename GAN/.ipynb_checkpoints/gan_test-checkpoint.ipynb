{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "(40, 40, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "workDir = '/mnt/c/users/llave/Documents/CMS/'\n",
    "data = np.load(workDir+'images.npy')\n",
    "print(len(data))\n",
    "print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def build_discriminator(img_shape):\n",
    "    input = Input(img_shape)\n",
    "    x = Conv2D(32*3, kernel_size=(3,3), strides=(2,2), padding=\"same\")(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64*3, kernel_size=(3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = ZeroPadding2D(padding=((0, 1), (0, 1)))(x)\n",
    "    x = (LeakyReLU(alpha=0.2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Conv2D(128*3, kernel_size=(3,3), strides=(2,2), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Conv2D(256*3, kernel_size=(3,3), strides=(1,1), padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(input, out)\n",
    "    print(\"-- Discriminator -- \")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_generator(noise_shape=(100,)):\n",
    "    input = Input(noise_shape)\n",
    "    x = Dense(128 * 10 * 10, activation=\"relu\")(input)\n",
    "    x = Reshape((10,10, 128))(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    #upsampling to 20x20\n",
    "    x = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    #upsampling to 40x40\n",
    "    x = Conv2DTranspose(64, (4,4),strides=(2,2), padding='same')(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Conv2D(3, kernel_size=3, padding=\"same\")(x)\n",
    "    out = Activation(\"tanh\")(x)\n",
    "    model = Model(input, out)\n",
    "    print(\"-- Generator -- \")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#plots events\n",
    "def plot_event(eventNum):\n",
    "    \n",
    "    x = events[eventNum]\n",
    "    \n",
    "    fig, axs = plt.subplots(1,3)\n",
    "    for i in range(3):\n",
    "        axs[i].imshow(x[:,:,i])\n",
    "        \n",
    "    axs[0].set_title(\"ECAL\")\n",
    "    axs[1].set_title(\"HCAL\")\n",
    "    axs[2].set_title(\"Muon\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#generates and saves 25 random images\n",
    "def save_imgs(generator, epoch, batch):\n",
    "    r, c = 5, 3\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, j], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(workDir+\"images/tracks_%d_%d.png\" % (epoch, batch))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Discriminator -- \n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 40, 40, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 20, 20, 96)        2688      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 20, 20, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20, 20, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 192)       166080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 11, 11, 192)       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 11, 11, 192)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 11, 11, 192)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 11, 11, 192)       768       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 384)         663936    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 6, 768)         2654976   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 768)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 6, 768)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 27648)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 27649     \n",
      "=================================================================\n",
      "Total params: 3,517,633\n",
      "Trainable params: 3,516,481\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "-- Generator -- \n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12800)             1292800   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 10, 10, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 20, 20, 128)       262272    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 40, 40, 64)        131136    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 40, 40, 3)         1731      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 40, 40, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,689,219\n",
      "Trainable params: 1,688,579\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build and compile discriminator and generator\n",
    "discriminator = build_discriminator(img_shape=(40,40, 3))\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                               optimizer=Adam(lr=0.0002, beta_1=0.5),\n",
    "                               metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "real = discriminator(img)\n",
    "combined = Model(z, real)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  204\n",
      "Number of Batches:  6\n",
      "Number of epochs:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llavezzo/.local/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 5/6 [D loss: 0.21, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.18]\n",
      "Epoch 1 Batch 5/6 [D loss: 0.39, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 2 Batch 5/6 [D loss: 0.36, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.15]\n",
      "Epoch 3 Batch 5/6 [D loss: 0.29, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 1.48]\n",
      "Epoch 4 Batch 5/6 [D loss: 0.40, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 1.95]\n",
      "Epoch 5 Batch 5/6 [D loss: 0.40, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.86]\n",
      "Epoch 6 Batch 5/6 [D loss: 0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 1.79]\n",
      "Epoch 7 Batch 5/6 [D loss: 0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.76]\n",
      "Epoch 8 Batch 5/6 [D loss: 0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.45]\n",
      "Epoch 9 Batch 5/6 [D loss: 0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.36]\n",
      "Epoch 10 Batch 5/6 [D loss: 0.60, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.13]\n",
      "Epoch 11 Batch 5/6 [D loss: 0.32, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.13]\n",
      "Epoch 12 Batch 5/6 [D loss: 0.11, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 2.21]\n",
      "Epoch 13 Batch 5/6 [D loss: 0.11, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.97]\n",
      "Epoch 14 Batch 5/6 [D loss: 0.11, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.74]\n",
      "Epoch 15 Batch 5/6 [D loss: 0.11, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.33]\n",
      "Epoch 16 Batch 5/6 [D loss: 0.11, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.33]\n",
      "Epoch 17 Batch 5/6 [D loss: 0.11, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 18 Batch 5/6 [D loss: 0.44, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 19 Batch 5/6 [D loss: 0.36, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 20 Batch 5/6 [D loss: 0.49, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 21 Batch 5/6 [D loss: 0.38, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 22 Batch 5/6 [D loss: 0.49, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 23 Batch 5/6 [D loss: -0.08, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 2.50]\n",
      "Epoch 24 Batch 5/6 [D loss: -0.08, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 1.40]\n",
      "Epoch 25 Batch 5/6 [D loss: -0.08, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.46]\n",
      "Epoch 26 Batch 5/6 [D loss: -0.08, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.39]\n",
      "Epoch 27 Batch 5/6 [D loss: -0.08, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.08]\n",
      "Epoch 28 Batch 5/6 [D loss: -0.08, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.20]\n",
      "Epoch 29 Batch 5/6 [D loss: 0.98, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.10]\n",
      "Epoch 30 Batch 5/6 [D loss: 0.10, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.10]\n",
      "Epoch 31 Batch 5/6 [D loss: 0.33, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.10]\n",
      "Epoch 32 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 2.82]\n",
      "Epoch 33 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 1.31]\n",
      "Epoch 34 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.61]\n",
      "Epoch 35 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.43]\n",
      "Epoch 36 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.24]\n",
      "Epoch 37 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.38]\n",
      "Epoch 38 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.07]\n",
      "Epoch 39 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.08]\n",
      "Epoch 40 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.23]\n",
      "Epoch 41 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.11]\n",
      "Epoch 42 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.13]\n",
      "Epoch 43 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.09]\n",
      "Epoch 44 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.32]\n",
      "Epoch 45 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.27]\n",
      "Epoch 46 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.23]\n",
      "Epoch 47 Batch 5/6 [D loss: -0.23, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.10]\n",
      "Epoch 48 Batch 5/6 [D loss: 0.57, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 49 Batch 5/6 [D loss: 0.52, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 50 Batch 5/6 [D loss: 0.38, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 51 Batch 5/6 [D loss: 0.73, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 52 Batch 5/6 [D loss: 0.42, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 53 Batch 5/6 [D loss: -0.10, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 54 Batch 5/6 [D loss: 0.67, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 55 Batch 5/6 [D loss: 0.34, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 56 Batch 5/6 [D loss: 0.21, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 57 Batch 5/6 [D loss: 0.07, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 58 Batch 5/6 [D loss: 0.03, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 59 Batch 5/6 [D loss: 0.14, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 60 Batch 5/6 [D loss: 0.48, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 61 Batch 5/6 [D loss: 0.22, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 62 Batch 5/6 [D loss: 1.49, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.30]\n",
      "Epoch 63 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 2.22]\n",
      "Epoch 64 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.86]\n",
      "Epoch 65 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.49]\n",
      "Epoch 66 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.12]\n",
      "Epoch 67 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.24]\n",
      "Epoch 68 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 69 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.18]\n",
      "Epoch 70 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.38]\n",
      "Epoch 71 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.20]\n",
      "Epoch 72 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.06]\n",
      "Epoch 73 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.04]\n",
      "Epoch 74 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.18]\n",
      "Epoch 75 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.13]\n",
      "Epoch 76 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.01]\n",
      "Epoch 77 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.01]\n",
      "Epoch 78 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.02]\n",
      "Epoch 79 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.07]\n",
      "Epoch 81 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.14]\n",
      "Epoch 82 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: -0.05]\n",
      "Epoch 83 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.11]\n",
      "Epoch 84 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.17]\n",
      "Epoch 85 Batch 5/6 [D loss: -0.51, acc avg: 0.00%] [D acc real: 0.00 D acc fake: 0.00], [G loss: 0.28]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eafa904d5ada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0md_loss_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "X_train = data\n",
    "\n",
    "epochs=1000\n",
    "batch_size=32\n",
    "save_interval=1\n",
    "\n",
    "num_examples = X_train.shape[0]\n",
    "num_batches = int(num_examples / float(batch_size))\n",
    "print('Number of examples: ', num_examples)\n",
    "print('Number of Batches: ', num_batches)\n",
    "print('Number of epochs: ', epochs)\n",
    "\n",
    "half_batch = int(batch_size / 2)\n",
    "\n",
    "d_loss = [10,10]\n",
    "g_loss = 10\n",
    "d_loss_array = []\n",
    "g_loss_array = []\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    for batch in range(num_batches):\n",
    "\n",
    "        # noise images for the batch\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        fake_images = generator.predict(noise)\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "        \n",
    "        #testing noisy labels\n",
    "        fake_labels = np.array([random.uniform(0,0.3) for i in range(half_batch)])\n",
    "\n",
    "        # real images for batch\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_images = X_train[idx]\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "        \n",
    "        #testing noisy labels\n",
    "        real_labels = np.array([random.uniform(0.7,1.3) for i in range(half_batch)])\n",
    "\n",
    "        # Train the discriminator (real classified as 1 and generated as 0)\n",
    "        if(d_loss[0] > g_loss or epoch < 5):\n",
    "            d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train the generator\n",
    "        if(g_loss >= d_loss[0] or epoch < 5):\n",
    "            \n",
    "            #testing noisy labels\n",
    "            labels = np.array([random.uniform(0.7,1.3) for i in range(batch_size)])\n",
    "            \n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            g_loss = combined.train_on_batch(noise, labels)\n",
    "            \n",
    "        d_loss_array.append(d_loss[0])\n",
    "        g_loss_array.append(g_loss)\n",
    "\n",
    "        # Plot the progress\n",
    "        if batch == num_batches-1:\n",
    "            print(\"Epoch %d Batch %d/%d [D loss: %.2f, acc avg: %.2f%%] [D acc real: %.2f D acc fake: %.2f], [G loss: %.2f]\" %\n",
    "                  (epoch, batch, num_batches, d_loss[0], 100 * d_loss[1], d_loss_real[1], d_loss_fake[1],g_loss))\n",
    "    \n",
    "    \n",
    "    if epoch % 5 == 0: save_imgs(generator, epoch, num_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_loss_array,label='d_loss')\n",
    "plt.plot(g_loss_array,label='g_loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
