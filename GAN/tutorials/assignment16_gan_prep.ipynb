{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs: Generative Adversarial Networks\n",
    "Imagine the following situation: you have a counterfeitor and a cop.   \n",
    "*  The counterfeitor makes a fake money sample.  Initially, the counterfeitor is very bad at this!\n",
    "*  The cop examines the fake money sample, compares it with real money, and she determines that the sample is fake.\n",
    "*  Based on the information from the cop, the counterfeitor improves his performance at making fake money and tries again.\n",
    "*  The cop examines the new fakes, and she determines - again - that they are indeed fake.\n",
    "*  The process continues, until at some point, the cop is no longer able to tell the difference between fake and real currency.   \n",
    "\n",
    "This is the basic outline of how a GAN works.   The GAN starts out with no knowledge of a sample, but through the use of a **generator** (the counterfeitor) and a **discriminator** (the cop), can end up with incredibly realistic versions of images, music, text, etc.\n",
    "\n",
    "GANs a represent an example of a **unsupervised learning**, in which the model learns features about a dataset without the data being labeled.\n",
    "\n",
    "GANS are a relatively new idea in machine learning, but they are quite interesting.  Yann LeCun - one of the major figures in machine learning - has called them “The coolest idea in deep learning in the last 20 years.”   Let see if we can understand how they work.\n",
    "\n",
    "For this workbook, I will rely heavily on the model described in Mike Bernico's book, [Deep Learning Quick Reference](https://www.amazon.com/Deep-Learning-Quick-Reference-optimizing-ebook/dp/B0791JRGPY)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple GAN Model\n",
    "A simple cartoon of a GAN is shown here (from [this link](https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394)):\n",
    "![gan](gan.png)\n",
    "\n",
    "The basic function of the GAN is the following:\n",
    "* The **generator** network is fed random noise, and outputs a *fake image*.\n",
    "* The **discriminator** network is fed a single image, as well as a label indicating that the image is either real or fake.   It should output 1 if the image is real, and 0 if the image is fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "We will design a GAN which will generate fake - but realistic - looking MNIST digits.  For the most part, we will be using techniques that you have seen before.   However, the interplay of the two networks of generator and discriminator make the successful training of a GAN very tricky.   So we will need to add some new features in order to make the training more stable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Discriminator\n",
    "\n",
    "Lets start with the discriminator.   We have made models like this before.  In fact, the basic idea here is fairly straightforward: a simple CNN which takes 28x28x1 images (like our MNIST dataset) and then produces a single output: 1 if the image is real, and 0 is the image is fake.  \n",
    "\n",
    "The discriminator we will use is shown below.   You will notice two new features:\n",
    "* A **Leaky ReLU** layer.  This replaces the ReLU activations that we have used before.  Leaky ReLU allows the pass of a small gradient signal for negative values. \n",
    "* A Batch Normalization layer.  Batch norm works by normalizing the input features of a layer to have zero mean and unit variance. Batch norm helps to deal with problems due to poor parameter initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def build_discriminator(img_shape):\n",
    "    input = Input(img_shape)\n",
    "    x = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = ZeroPadding2D(padding=((0, 1), (0, 1)))(x)\n",
    "    x = (LeakyReLU(alpha=0.2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Conv2D(256, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(input, out)\n",
    "    print(\"-- Discriminator -- \")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Generator\n",
    "The generator takes a random vector - in this case a vector of length 100 - and via a series of Keras layers produces an image - in our case a 28x28x1 image.   It uses the Batch Normalization layer, as well as an **UpSampling** layer.  We have previosly used UpSampling layers when we first introduced CNN autoencoders.   The UpSampling layer repeats the rows and columns of the data by size[0] and size[1] respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_generator(noise_shape=(100,)):\n",
    "    input = Input(noise_shape)\n",
    "    x = Dense(128 * 7 * 7, activation=\"relu\")(input)\n",
    "    x = Reshape((7, 7, 128))(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(128, kernel_size=3, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(64, kernel_size=3, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Conv2D(1, kernel_size=3, padding=\"same\")(x)\n",
    "    out = Activation(\"tanh\")(x)\n",
    "    model = Model(input, out)\n",
    "    print(\"-- Generator -- \")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "The following is a helper function to get the MNIST data.   We will end up just using the training image data, and not the test data.   **We won't use the MNIST labels at all**.  The key thing we know about the MNIST training images is that they are **real**.   We will label images below, but the only labels we need are whether the images are **real** (which is only the case if they come from MNIST) or **fake** (which is only the case if the images come from our generator model above).   We don't need to know if the real image is a 0,1,..,9,  just that it is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "    return X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for displaying the generator images\n",
    "As we train our generator, we will want to inspect the images to see how close they appear to emulating real images.   We will call the function a number of times during each epoch that we train the networks.    The code below uses 25 random vectors of length 100, feeds them into the current version of the generator, and makes 25 fake images.   It does not display them to the screen - it saves them to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(generator, epoch, batch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d_%d.png\" % (epoch, batch))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and compile the discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Discriminator -- \n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 392,705\n",
      "Trainable params: 392,321\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "-- Generator -- \n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,705\n",
      "Trainable params: 856,065\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator(img_shape=(28, 28, 1))\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                               optimizer=Adam(lr=0.0002, beta_1=0.5),\n",
    "                               metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the discriminator\n",
    "We have everything we need to train the discriminator: we have real images (from MNIST) and fake images (which will come from the generator).  So to train the discriminator, we simply have to feed it batches of labeled (remeber just if they are real or fake) images.   But how do we train the generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the generator\n",
    "The generator needs information to determine how poorly (or well) its fake images are.  To do this, we will use a clever trick:  we will use the output of the same discriminator above!  \n",
    "\n",
    "To do this, we will need to make a **third** model, which below we call the **combined** model :\n",
    "*  The combined model uses the generator output, and feeds it into the discriminator.  For this step, we will tell the combined model that these fake images are actually real.\n",
    "*  During the **generator training process**, we will need to freeze the weights of the discriminator, so that only the weights of the generator are adjusted.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "real = discriminator(img)\n",
    "combined = Model(z, real)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training loop\n",
    "The training loop below contains the logic of how we fit all 3 models (really just the two: the generator and the discriminator).  Some things to pay attention to in the code below:\n",
    "\n",
    "1.  There is an outer loop over epochs, and an inner loop over batches.   With a batch size of 32 and the 60k MNIST images, this means we will run the inner loop 1875 times per epoch.\n",
    "2.  During each batch iteration, we feed the disrimnator 16 real images and 16 fake images.  The labels for these are 1 and 0 respectively.  Notice that we randomly sample (with replacement) the real images.   So we are not using the full data each epoch!   Might be a good idea to revisit this if we were doing it for real....\n",
    "3.  In the first half of this loop, we run **train_on_batch** to train the **discriminator only**.   We use train_on_batch rather than **fit** because we need to control how we feed data to the model when fitting - train_on_batch allows us to do that.\n",
    "4.  **After** this, we then send 32 new **fake** images to the **combined** model.   These images are labeled as **real** (even though they are fake).   Remember that the **same** discriminator model is used as in step 3, but for this part of the loop the discriminator weights are frozen to the values that they had at step 3.  **Only the generator** weights are changed during this step.   Since we are asking the disciminator output to be 1, we are adjusting the generator weights so that the generator gets better at producing images which the discriminator will believe are real.\n",
    "5.  Every 50 batches within each epoch, we generate 50 fake images, using the current version of the generator model.\n",
    "6.  As the training progresses through epochs/batches, examine how the discriminatr loss and accuracy, as well as the generator loss change.  Note: train_on_batch for the discriminator returns both the loss as well as the metric (which is the accuracy in this case).\n",
    "\n",
    "\n",
    "**NOTE**: Each epoch will take **alot** of time!   So you probably will want to stop it after two epoch or so.   If you want to see some cool images after 1,2,3,4 epochs, submit the script  **pbs_gan_gpu.sh** to the pbs batch system.   In about 30 minutes you will have some amazingly real digit images in the images/ directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  60000\n",
      "Number of Batches:  1875\n",
      "Number of epochs:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llavezzo/.local/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0/1875 [D loss: 1.02, acc avg: 40.62%] [D acc real: 0.81 D acc fake: 0.00], [G loss: 0.71]\n",
      "Epoch 0 Batch 1/1875 [D loss: 0.58, acc avg: 71.88%] [D acc real: 1.00 D acc fake: 0.44], [G loss: 0.76]\n",
      "Epoch 0 Batch 2/1875 [D loss: 0.43, acc avg: 81.25%] [D acc real: 0.94 D acc fake: 0.69], [G loss: 0.75]\n",
      "Epoch 0 Batch 3/1875 [D loss: 0.22, acc avg: 93.75%] [D acc real: 0.94 D acc fake: 0.94], [G loss: 0.83]\n",
      "Epoch 0 Batch 4/1875 [D loss: 0.15, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 0.74]\n",
      "Epoch 0 Batch 5/1875 [D loss: 0.14, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 0.76]\n",
      "Epoch 0 Batch 6/1875 [D loss: 0.09, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 0.68]\n",
      "Epoch 0 Batch 7/1875 [D loss: 0.12, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 0.65]\n",
      "Epoch 0 Batch 8/1875 [D loss: 0.06, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 0.59]\n",
      "Epoch 0 Batch 9/1875 [D loss: 0.15, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 0.80]\n",
      "Epoch 0 Batch 10/1875 [D loss: 0.21, acc avg: 90.62%] [D acc real: 1.00 D acc fake: 0.81], [G loss: 0.70]\n",
      "Epoch 0 Batch 11/1875 [D loss: 0.26, acc avg: 90.62%] [D acc real: 1.00 D acc fake: 0.81], [G loss: 0.96]\n",
      "Epoch 0 Batch 12/1875 [D loss: 0.26, acc avg: 93.75%] [D acc real: 1.00 D acc fake: 0.88], [G loss: 1.06]\n",
      "Epoch 0 Batch 13/1875 [D loss: 0.20, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 1.43]\n",
      "Epoch 0 Batch 14/1875 [D loss: 0.22, acc avg: 93.75%] [D acc real: 1.00 D acc fake: 0.88], [G loss: 1.76]\n",
      "Epoch 0 Batch 15/1875 [D loss: 0.24, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 1.75]\n",
      "Epoch 0 Batch 16/1875 [D loss: 0.26, acc avg: 90.62%] [D acc real: 1.00 D acc fake: 0.81], [G loss: 2.25]\n",
      "Epoch 0 Batch 17/1875 [D loss: 0.27, acc avg: 84.38%] [D acc real: 0.88 D acc fake: 0.81], [G loss: 2.15]\n",
      "Epoch 0 Batch 18/1875 [D loss: 0.29, acc avg: 87.50%] [D acc real: 1.00 D acc fake: 0.75], [G loss: 2.12]\n",
      "Epoch 0 Batch 19/1875 [D loss: 0.40, acc avg: 87.50%] [D acc real: 0.94 D acc fake: 0.81], [G loss: 2.52]\n",
      "Epoch 0 Batch 20/1875 [D loss: 0.26, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 3.36]\n",
      "Epoch 0 Batch 21/1875 [D loss: 0.22, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 2.77]\n",
      "Epoch 0 Batch 22/1875 [D loss: 0.25, acc avg: 90.62%] [D acc real: 0.94 D acc fake: 0.88], [G loss: 3.05]\n",
      "Epoch 0 Batch 23/1875 [D loss: 0.25, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 2.89]\n",
      "Epoch 0 Batch 24/1875 [D loss: 0.40, acc avg: 84.38%] [D acc real: 0.94 D acc fake: 0.75], [G loss: 2.83]\n",
      "Epoch 0 Batch 25/1875 [D loss: 0.32, acc avg: 90.62%] [D acc real: 1.00 D acc fake: 0.81], [G loss: 2.98]\n",
      "Epoch 0 Batch 26/1875 [D loss: 0.16, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 3.60]\n",
      "Epoch 0 Batch 27/1875 [D loss: 0.23, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 3.25]\n",
      "Epoch 0 Batch 28/1875 [D loss: 0.18, acc avg: 93.75%] [D acc real: 1.00 D acc fake: 0.88], [G loss: 3.05]\n",
      "Epoch 0 Batch 29/1875 [D loss: 0.12, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 2.98]\n",
      "Epoch 0 Batch 30/1875 [D loss: 0.17, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 3.10]\n",
      "Epoch 0 Batch 31/1875 [D loss: 0.20, acc avg: 90.62%] [D acc real: 1.00 D acc fake: 0.81], [G loss: 3.42]\n",
      "Epoch 0 Batch 32/1875 [D loss: 0.07, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.75]\n",
      "Epoch 0 Batch 33/1875 [D loss: 0.11, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.37]\n",
      "Epoch 0 Batch 34/1875 [D loss: 0.09, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.13]\n",
      "Epoch 0 Batch 35/1875 [D loss: 0.10, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.09]\n",
      "Epoch 0 Batch 36/1875 [D loss: 0.07, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.17]\n",
      "Epoch 0 Batch 37/1875 [D loss: 0.04, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.32]\n",
      "Epoch 0 Batch 38/1875 [D loss: 0.09, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 2.95]\n",
      "Epoch 0 Batch 39/1875 [D loss: 0.07, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.15]\n",
      "Epoch 0 Batch 40/1875 [D loss: 0.06, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.29]\n",
      "Epoch 0 Batch 41/1875 [D loss: 0.09, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.42]\n",
      "Epoch 0 Batch 42/1875 [D loss: 0.07, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 2.95]\n",
      "Epoch 0 Batch 43/1875 [D loss: 0.06, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.06]\n",
      "Epoch 0 Batch 44/1875 [D loss: 0.13, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.00]\n",
      "Epoch 0 Batch 45/1875 [D loss: 0.09, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 4.04]\n",
      "Epoch 0 Batch 46/1875 [D loss: 0.05, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.32]\n",
      "Epoch 0 Batch 47/1875 [D loss: 0.05, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.38]\n",
      "Epoch 0 Batch 48/1875 [D loss: 0.06, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.60]\n",
      "Epoch 0 Batch 49/1875 [D loss: 0.06, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.68]\n",
      "Epoch 0 Batch 50/1875 [D loss: 0.06, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.33]\n",
      "Epoch 0 Batch 51/1875 [D loss: 0.03, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.47]\n",
      "Epoch 0 Batch 52/1875 [D loss: 0.12, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 3.83]\n",
      "Epoch 0 Batch 53/1875 [D loss: 0.08, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.20]\n",
      "Epoch 0 Batch 54/1875 [D loss: 0.02, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.05]\n",
      "Epoch 0 Batch 55/1875 [D loss: 0.14, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 4.29]\n",
      "Epoch 0 Batch 56/1875 [D loss: 0.09, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.09]\n",
      "Epoch 0 Batch 57/1875 [D loss: 0.08, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.57]\n",
      "Epoch 0 Batch 58/1875 [D loss: 0.18, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 2.97]\n",
      "Epoch 0 Batch 59/1875 [D loss: 0.04, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.30]\n",
      "Epoch 0 Batch 60/1875 [D loss: 0.03, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.16]\n",
      "Epoch 0 Batch 61/1875 [D loss: 0.01, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.68]\n",
      "Epoch 0 Batch 62/1875 [D loss: 0.07, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.88]\n",
      "Epoch 0 Batch 63/1875 [D loss: 0.08, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.62]\n",
      "Epoch 0 Batch 64/1875 [D loss: 0.02, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.19]\n",
      "Epoch 0 Batch 65/1875 [D loss: 0.11, acc avg: 93.75%] [D acc real: 0.88 D acc fake: 1.00], [G loss: 2.83]\n",
      "Epoch 0 Batch 66/1875 [D loss: 0.02, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 2.97]\n",
      "Epoch 0 Batch 67/1875 [D loss: 0.05, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 2.66]\n",
      "Epoch 0 Batch 68/1875 [D loss: 0.06, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.48]\n",
      "Epoch 0 Batch 69/1875 [D loss: 0.10, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 2.69]\n",
      "Epoch 0 Batch 70/1875 [D loss: 0.16, acc avg: 93.75%] [D acc real: 1.00 D acc fake: 0.88], [G loss: 4.14]\n",
      "Epoch 0 Batch 71/1875 [D loss: 0.02, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.58]\n",
      "Epoch 0 Batch 72/1875 [D loss: 0.04, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.32]\n",
      "Epoch 0 Batch 73/1875 [D loss: 0.01, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.59]\n",
      "Epoch 0 Batch 74/1875 [D loss: 0.05, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.33]\n",
      "Epoch 0 Batch 75/1875 [D loss: 0.02, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.79]\n",
      "Epoch 0 Batch 76/1875 [D loss: 0.12, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 2.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 77/1875 [D loss: 0.04, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.02]\n",
      "Epoch 0 Batch 78/1875 [D loss: 0.03, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.09]\n",
      "Epoch 0 Batch 79/1875 [D loss: 0.04, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 2.90]\n",
      "Epoch 0 Batch 80/1875 [D loss: 0.01, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.57]\n",
      "Epoch 0 Batch 81/1875 [D loss: 0.11, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 3.83]\n",
      "Epoch 0 Batch 82/1875 [D loss: 0.04, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 2.54]\n",
      "Epoch 0 Batch 83/1875 [D loss: 0.10, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 4.45]\n",
      "Epoch 0 Batch 84/1875 [D loss: 0.16, acc avg: 96.88%] [D acc real: 1.00 D acc fake: 0.94], [G loss: 3.96]\n",
      "Epoch 0 Batch 85/1875 [D loss: 0.11, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 3.42]\n",
      "Epoch 0 Batch 86/1875 [D loss: 0.01, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.84]\n",
      "Epoch 0 Batch 87/1875 [D loss: 0.16, acc avg: 90.62%] [D acc real: 1.00 D acc fake: 0.81], [G loss: 4.65]\n",
      "Epoch 0 Batch 88/1875 [D loss: 0.01, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 6.28]\n",
      "Epoch 0 Batch 89/1875 [D loss: 0.41, acc avg: 84.38%] [D acc real: 0.69 D acc fake: 1.00], [G loss: 1.40]\n",
      "Epoch 0 Batch 90/1875 [D loss: 0.64, acc avg: 62.50%] [D acc real: 1.00 D acc fake: 0.25], [G loss: 5.58]\n",
      "Epoch 0 Batch 91/1875 [D loss: 0.17, acc avg: 90.62%] [D acc real: 0.81 D acc fake: 1.00], [G loss: 5.31]\n",
      "Epoch 0 Batch 92/1875 [D loss: 0.62, acc avg: 65.62%] [D acc real: 0.62 D acc fake: 0.69], [G loss: 4.59]\n",
      "Epoch 0 Batch 93/1875 [D loss: 0.03, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.93]\n",
      "Epoch 0 Batch 94/1875 [D loss: 0.49, acc avg: 71.88%] [D acc real: 0.88 D acc fake: 0.56], [G loss: 4.34]\n",
      "Epoch 0 Batch 95/1875 [D loss: 0.22, acc avg: 90.62%] [D acc real: 0.81 D acc fake: 1.00], [G loss: 3.55]\n",
      "Epoch 0 Batch 96/1875 [D loss: 0.38, acc avg: 81.25%] [D acc real: 0.94 D acc fake: 0.69], [G loss: 5.11]\n",
      "Epoch 0 Batch 97/1875 [D loss: 0.04, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 6.91]\n",
      "Epoch 0 Batch 98/1875 [D loss: 0.51, acc avg: 68.75%] [D acc real: 0.94 D acc fake: 0.44], [G loss: 7.59]\n",
      "Epoch 0 Batch 99/1875 [D loss: 0.32, acc avg: 84.38%] [D acc real: 0.69 D acc fake: 1.00], [G loss: 4.67]\n",
      "Epoch 0 Batch 100/1875 [D loss: 0.06, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 3.67]\n",
      "Epoch 0 Batch 101/1875 [D loss: 0.30, acc avg: 84.38%] [D acc real: 1.00 D acc fake: 0.69], [G loss: 5.76]\n",
      "Epoch 0 Batch 102/1875 [D loss: 0.07, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 6.86]\n",
      "Epoch 0 Batch 103/1875 [D loss: 0.15, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.60]\n",
      "Epoch 0 Batch 104/1875 [D loss: 0.48, acc avg: 75.00%] [D acc real: 0.75 D acc fake: 0.75], [G loss: 3.05]\n",
      "Epoch 0 Batch 105/1875 [D loss: 0.18, acc avg: 90.62%] [D acc real: 0.94 D acc fake: 0.88], [G loss: 4.06]\n",
      "Epoch 0 Batch 106/1875 [D loss: 0.04, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 4.03]\n",
      "Epoch 0 Batch 107/1875 [D loss: 0.02, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 5.25]\n",
      "Epoch 0 Batch 108/1875 [D loss: 0.07, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 3.65]\n",
      "Epoch 0 Batch 109/1875 [D loss: 0.16, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 2.43]\n",
      "Epoch 0 Batch 110/1875 [D loss: 0.07, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 2.98]\n",
      "Epoch 0 Batch 111/1875 [D loss: 0.38, acc avg: 87.50%] [D acc real: 0.88 D acc fake: 0.88], [G loss: 2.47]\n",
      "Epoch 0 Batch 112/1875 [D loss: 0.18, acc avg: 93.75%] [D acc real: 1.00 D acc fake: 0.88], [G loss: 4.12]\n",
      "Epoch 0 Batch 113/1875 [D loss: 0.07, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 5.28]\n",
      "Epoch 0 Batch 114/1875 [D loss: 0.98, acc avg: 40.62%] [D acc real: 0.69 D acc fake: 0.12], [G loss: 5.03]\n",
      "Epoch 0 Batch 115/1875 [D loss: 0.15, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 4.72]\n",
      "Epoch 0 Batch 116/1875 [D loss: 1.19, acc avg: 37.50%] [D acc real: 0.44 D acc fake: 0.31], [G loss: 3.08]\n",
      "Epoch 0 Batch 117/1875 [D loss: 0.11, acc avg: 100.00%] [D acc real: 1.00 D acc fake: 1.00], [G loss: 5.27]\n",
      "Epoch 0 Batch 118/1875 [D loss: 0.21, acc avg: 90.62%] [D acc real: 0.81 D acc fake: 1.00], [G loss: 3.11]\n",
      "Epoch 0 Batch 119/1875 [D loss: 0.23, acc avg: 87.50%] [D acc real: 1.00 D acc fake: 0.75], [G loss: 5.48]\n",
      "Epoch 0 Batch 120/1875 [D loss: 2.04, acc avg: 31.25%] [D acc real: 0.06 D acc fake: 0.56], [G loss: 1.56]\n",
      "Epoch 0 Batch 121/1875 [D loss: 0.18, acc avg: 93.75%] [D acc real: 1.00 D acc fake: 0.88], [G loss: 2.59]\n",
      "Epoch 0 Batch 122/1875 [D loss: 0.09, acc avg: 93.75%] [D acc real: 1.00 D acc fake: 0.88], [G loss: 3.67]\n",
      "Epoch 0 Batch 123/1875 [D loss: 0.18, acc avg: 93.75%] [D acc real: 0.88 D acc fake: 1.00], [G loss: 3.39]\n",
      "Epoch 0 Batch 124/1875 [D loss: 0.36, acc avg: 78.12%] [D acc real: 1.00 D acc fake: 0.56], [G loss: 4.73]\n",
      "Epoch 0 Batch 125/1875 [D loss: 0.74, acc avg: 65.62%] [D acc real: 0.62 D acc fake: 0.69], [G loss: 3.95]\n",
      "Epoch 0 Batch 126/1875 [D loss: 0.19, acc avg: 93.75%] [D acc real: 1.00 D acc fake: 0.88], [G loss: 4.25]\n",
      "Epoch 0 Batch 127/1875 [D loss: 0.63, acc avg: 62.50%] [D acc real: 0.44 D acc fake: 0.81], [G loss: 3.33]\n",
      "Epoch 0 Batch 128/1875 [D loss: 0.52, acc avg: 71.88%] [D acc real: 0.88 D acc fake: 0.56], [G loss: 3.67]\n",
      "Epoch 0 Batch 129/1875 [D loss: 0.48, acc avg: 75.00%] [D acc real: 0.69 D acc fake: 0.81], [G loss: 2.58]\n",
      "Epoch 0 Batch 130/1875 [D loss: 1.47, acc avg: 25.00%] [D acc real: 0.38 D acc fake: 0.12], [G loss: 3.10]\n",
      "Epoch 0 Batch 131/1875 [D loss: 0.27, acc avg: 87.50%] [D acc real: 0.81 D acc fake: 0.94], [G loss: 4.17]\n",
      "Epoch 0 Batch 132/1875 [D loss: 0.60, acc avg: 65.62%] [D acc real: 0.62 D acc fake: 0.69], [G loss: 3.38]\n",
      "Epoch 0 Batch 133/1875 [D loss: 0.34, acc avg: 87.50%] [D acc real: 0.81 D acc fake: 0.94], [G loss: 2.52]\n",
      "Epoch 0 Batch 134/1875 [D loss: 0.43, acc avg: 75.00%] [D acc real: 0.88 D acc fake: 0.62], [G loss: 3.35]\n",
      "Epoch 0 Batch 135/1875 [D loss: 0.40, acc avg: 87.50%] [D acc real: 0.81 D acc fake: 0.94], [G loss: 3.71]\n",
      "Epoch 0 Batch 136/1875 [D loss: 0.72, acc avg: 71.88%] [D acc real: 0.56 D acc fake: 0.88], [G loss: 2.25]\n",
      "Epoch 0 Batch 137/1875 [D loss: 0.80, acc avg: 65.62%] [D acc real: 0.94 D acc fake: 0.38], [G loss: 4.30]\n",
      "Epoch 0 Batch 138/1875 [D loss: 0.47, acc avg: 78.12%] [D acc real: 0.62 D acc fake: 0.94], [G loss: 3.64]\n",
      "Epoch 0 Batch 139/1875 [D loss: 0.43, acc avg: 81.25%] [D acc real: 0.69 D acc fake: 0.94], [G loss: 2.17]\n",
      "Epoch 0 Batch 140/1875 [D loss: 0.51, acc avg: 75.00%] [D acc real: 0.94 D acc fake: 0.56], [G loss: 3.17]\n",
      "Epoch 0 Batch 141/1875 [D loss: 0.44, acc avg: 75.00%] [D acc real: 0.81 D acc fake: 0.69], [G loss: 3.55]\n",
      "Epoch 0 Batch 142/1875 [D loss: 1.18, acc avg: 28.12%] [D acc real: 0.44 D acc fake: 0.12], [G loss: 3.66]\n",
      "Epoch 0 Batch 143/1875 [D loss: 0.83, acc avg: 50.00%] [D acc real: 0.38 D acc fake: 0.62], [G loss: 3.42]\n",
      "Epoch 0 Batch 144/1875 [D loss: 0.17, acc avg: 96.88%] [D acc real: 0.94 D acc fake: 1.00], [G loss: 3.94]\n",
      "Epoch 0 Batch 145/1875 [D loss: 0.72, acc avg: 56.25%] [D acc real: 0.38 D acc fake: 0.75], [G loss: 2.32]\n",
      "Epoch 0 Batch 146/1875 [D loss: 0.52, acc avg: 75.00%] [D acc real: 0.81 D acc fake: 0.69], [G loss: 2.84]\n",
      "Epoch 0 Batch 147/1875 [D loss: 0.36, acc avg: 81.25%] [D acc real: 0.81 D acc fake: 0.81], [G loss: 2.35]\n",
      "Epoch 0 Batch 148/1875 [D loss: 0.27, acc avg: 87.50%] [D acc real: 0.94 D acc fake: 0.81], [G loss: 3.57]\n",
      "Epoch 0 Batch 149/1875 [D loss: 0.54, acc avg: 75.00%] [D acc real: 0.69 D acc fake: 0.81], [G loss: 2.36]\n",
      "Epoch 0 Batch 150/1875 [D loss: 0.89, acc avg: 59.38%] [D acc real: 0.75 D acc fake: 0.44], [G loss: 2.56]\n",
      "Epoch 0 Batch 151/1875 [D loss: 0.41, acc avg: 78.12%] [D acc real: 0.88 D acc fake: 0.69], [G loss: 3.96]\n",
      "Epoch 0 Batch 152/1875 [D loss: 0.54, acc avg: 68.75%] [D acc real: 0.44 D acc fake: 0.94], [G loss: 2.79]\n",
      "Epoch 0 Batch 153/1875 [D loss: 0.52, acc avg: 78.12%] [D acc real: 0.81 D acc fake: 0.75], [G loss: 1.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 154/1875 [D loss: 0.66, acc avg: 75.00%] [D acc real: 0.81 D acc fake: 0.69], [G loss: 1.52]\n",
      "Epoch 0 Batch 155/1875 [D loss: 0.66, acc avg: 68.75%] [D acc real: 0.81 D acc fake: 0.56], [G loss: 2.81]\n",
      "Epoch 0 Batch 156/1875 [D loss: 0.85, acc avg: 50.00%] [D acc real: 0.38 D acc fake: 0.62], [G loss: 2.53]\n",
      "Epoch 0 Batch 157/1875 [D loss: 0.85, acc avg: 59.38%] [D acc real: 0.81 D acc fake: 0.38], [G loss: 4.09]\n",
      "Epoch 0 Batch 158/1875 [D loss: 1.25, acc avg: 46.88%] [D acc real: 0.12 D acc fake: 0.81], [G loss: 1.88]\n",
      "Epoch 0 Batch 159/1875 [D loss: 1.06, acc avg: 59.38%] [D acc real: 1.00 D acc fake: 0.19], [G loss: 3.73]\n",
      "Epoch 0 Batch 160/1875 [D loss: 0.80, acc avg: 62.50%] [D acc real: 0.31 D acc fake: 0.94], [G loss: 2.74]\n",
      "Epoch 0 Batch 161/1875 [D loss: 0.62, acc avg: 71.88%] [D acc real: 0.75 D acc fake: 0.69], [G loss: 1.77]\n",
      "Epoch 0 Batch 162/1875 [D loss: 0.77, acc avg: 59.38%] [D acc real: 0.75 D acc fake: 0.44], [G loss: 2.65]\n",
      "Epoch 0 Batch 163/1875 [D loss: 0.70, acc avg: 65.62%] [D acc real: 0.62 D acc fake: 0.69], [G loss: 2.11]\n",
      "Epoch 0 Batch 164/1875 [D loss: 0.49, acc avg: 78.12%] [D acc real: 0.81 D acc fake: 0.75], [G loss: 2.70]\n",
      "Epoch 0 Batch 165/1875 [D loss: 0.35, acc avg: 84.38%] [D acc real: 0.88 D acc fake: 0.81], [G loss: 2.95]\n",
      "Epoch 0 Batch 166/1875 [D loss: 0.45, acc avg: 75.00%] [D acc real: 0.75 D acc fake: 0.75], [G loss: 2.27]\n",
      "Epoch 0 Batch 167/1875 [D loss: 0.83, acc avg: 59.38%] [D acc real: 0.50 D acc fake: 0.69], [G loss: 1.43]\n",
      "Epoch 0 Batch 168/1875 [D loss: 0.61, acc avg: 65.62%] [D acc real: 0.81 D acc fake: 0.50], [G loss: 3.03]\n",
      "Epoch 0 Batch 169/1875 [D loss: 0.83, acc avg: 62.50%] [D acc real: 0.31 D acc fake: 0.94], [G loss: 1.43]\n",
      "Epoch 0 Batch 170/1875 [D loss: 0.65, acc avg: 68.75%] [D acc real: 0.88 D acc fake: 0.50], [G loss: 2.54]\n",
      "Epoch 0 Batch 171/1875 [D loss: 0.58, acc avg: 65.62%] [D acc real: 0.75 D acc fake: 0.56], [G loss: 2.97]\n",
      "Epoch 0 Batch 172/1875 [D loss: 0.73, acc avg: 65.62%] [D acc real: 0.56 D acc fake: 0.75], [G loss: 2.05]\n",
      "Epoch 0 Batch 173/1875 [D loss: 1.11, acc avg: 46.88%] [D acc real: 0.62 D acc fake: 0.31], [G loss: 2.27]\n",
      "Epoch 0 Batch 174/1875 [D loss: 1.01, acc avg: 43.75%] [D acc real: 0.44 D acc fake: 0.44], [G loss: 2.22]\n",
      "Epoch 0 Batch 175/1875 [D loss: 0.74, acc avg: 62.50%] [D acc real: 0.69 D acc fake: 0.56], [G loss: 2.61]\n",
      "Epoch 0 Batch 176/1875 [D loss: 0.74, acc avg: 65.62%] [D acc real: 0.75 D acc fake: 0.56], [G loss: 3.14]\n",
      "Epoch 0 Batch 177/1875 [D loss: 1.29, acc avg: 31.25%] [D acc real: 0.12 D acc fake: 0.50], [G loss: 1.85]\n",
      "Epoch 0 Batch 178/1875 [D loss: 1.00, acc avg: 50.00%] [D acc real: 0.56 D acc fake: 0.44], [G loss: 2.20]\n",
      "Epoch 0 Batch 179/1875 [D loss: 0.94, acc avg: 56.25%] [D acc real: 0.62 D acc fake: 0.50], [G loss: 2.58]\n",
      "Epoch 0 Batch 180/1875 [D loss: 1.07, acc avg: 50.00%] [D acc real: 0.44 D acc fake: 0.56], [G loss: 2.04]\n",
      "Epoch 0 Batch 181/1875 [D loss: 0.75, acc avg: 56.25%] [D acc real: 0.38 D acc fake: 0.75], [G loss: 1.62]\n",
      "Epoch 0 Batch 182/1875 [D loss: 0.50, acc avg: 78.12%] [D acc real: 0.81 D acc fake: 0.75], [G loss: 1.88]\n",
      "Epoch 0 Batch 183/1875 [D loss: 0.94, acc avg: 46.88%] [D acc real: 0.44 D acc fake: 0.50], [G loss: 1.41]\n",
      "Epoch 0 Batch 184/1875 [D loss: 0.86, acc avg: 56.25%] [D acc real: 0.56 D acc fake: 0.56], [G loss: 1.75]\n",
      "Epoch 0 Batch 185/1875 [D loss: 0.88, acc avg: 59.38%] [D acc real: 0.62 D acc fake: 0.56], [G loss: 2.07]\n",
      "Epoch 0 Batch 186/1875 [D loss: 0.98, acc avg: 40.62%] [D acc real: 0.38 D acc fake: 0.44], [G loss: 1.96]\n",
      "Epoch 0 Batch 187/1875 [D loss: 0.85, acc avg: 46.88%] [D acc real: 0.56 D acc fake: 0.38], [G loss: 2.11]\n",
      "Epoch 0 Batch 188/1875 [D loss: 1.11, acc avg: 31.25%] [D acc real: 0.19 D acc fake: 0.44], [G loss: 1.37]\n",
      "Epoch 0 Batch 189/1875 [D loss: 1.03, acc avg: 37.50%] [D acc real: 0.50 D acc fake: 0.25], [G loss: 1.76]\n",
      "Epoch 0 Batch 190/1875 [D loss: 0.78, acc avg: 65.62%] [D acc real: 0.62 D acc fake: 0.69], [G loss: 1.97]\n",
      "Epoch 0 Batch 191/1875 [D loss: 0.80, acc avg: 59.38%] [D acc real: 0.44 D acc fake: 0.75], [G loss: 1.19]\n",
      "Epoch 0 Batch 192/1875 [D loss: 0.62, acc avg: 75.00%] [D acc real: 0.81 D acc fake: 0.69], [G loss: 1.72]\n",
      "Epoch 0 Batch 193/1875 [D loss: 0.82, acc avg: 56.25%] [D acc real: 0.56 D acc fake: 0.56], [G loss: 1.61]\n",
      "Epoch 0 Batch 194/1875 [D loss: 1.12, acc avg: 28.12%] [D acc real: 0.25 D acc fake: 0.31], [G loss: 1.70]\n",
      "Epoch 0 Batch 195/1875 [D loss: 0.64, acc avg: 59.38%] [D acc real: 0.38 D acc fake: 0.81], [G loss: 1.98]\n",
      "Epoch 0 Batch 196/1875 [D loss: 0.77, acc avg: 62.50%] [D acc real: 0.56 D acc fake: 0.69], [G loss: 1.32]\n",
      "Epoch 0 Batch 197/1875 [D loss: 0.76, acc avg: 59.38%] [D acc real: 0.50 D acc fake: 0.69], [G loss: 1.98]\n",
      "Epoch 0 Batch 198/1875 [D loss: 0.71, acc avg: 68.75%] [D acc real: 0.62 D acc fake: 0.75], [G loss: 1.70]\n",
      "Epoch 0 Batch 199/1875 [D loss: 0.87, acc avg: 56.25%] [D acc real: 0.50 D acc fake: 0.62], [G loss: 1.42]\n",
      "Epoch 0 Batch 200/1875 [D loss: 0.49, acc avg: 81.25%] [D acc real: 0.81 D acc fake: 0.81], [G loss: 1.55]\n",
      "Epoch 0 Batch 201/1875 [D loss: 0.72, acc avg: 65.62%] [D acc real: 0.50 D acc fake: 0.81], [G loss: 1.33]\n",
      "Epoch 0 Batch 202/1875 [D loss: 0.54, acc avg: 65.62%] [D acc real: 0.69 D acc fake: 0.62], [G loss: 1.42]\n",
      "Epoch 0 Batch 203/1875 [D loss: 0.91, acc avg: 50.00%] [D acc real: 0.69 D acc fake: 0.31], [G loss: 1.85]\n",
      "Epoch 0 Batch 204/1875 [D loss: 0.49, acc avg: 78.12%] [D acc real: 0.69 D acc fake: 0.88], [G loss: 1.73]\n",
      "Epoch 0 Batch 205/1875 [D loss: 0.77, acc avg: 62.50%] [D acc real: 0.56 D acc fake: 0.69], [G loss: 1.22]\n",
      "Epoch 0 Batch 206/1875 [D loss: 1.01, acc avg: 50.00%] [D acc real: 0.50 D acc fake: 0.50], [G loss: 1.28]\n",
      "Epoch 0 Batch 207/1875 [D loss: 0.71, acc avg: 68.75%] [D acc real: 0.81 D acc fake: 0.56], [G loss: 1.57]\n",
      "Epoch 0 Batch 208/1875 [D loss: 1.02, acc avg: 46.88%] [D acc real: 0.44 D acc fake: 0.50], [G loss: 1.46]\n",
      "Epoch 0 Batch 209/1875 [D loss: 0.78, acc avg: 53.12%] [D acc real: 0.62 D acc fake: 0.44], [G loss: 1.82]\n",
      "Epoch 0 Batch 210/1875 [D loss: 0.89, acc avg: 53.12%] [D acc real: 0.44 D acc fake: 0.62], [G loss: 1.69]\n",
      "Epoch 0 Batch 211/1875 [D loss: 0.73, acc avg: 59.38%] [D acc real: 0.50 D acc fake: 0.69], [G loss: 1.64]\n",
      "Epoch 0 Batch 212/1875 [D loss: 0.68, acc avg: 59.38%] [D acc real: 0.62 D acc fake: 0.56], [G loss: 1.79]\n",
      "Epoch 0 Batch 213/1875 [D loss: 0.86, acc avg: 46.88%] [D acc real: 0.25 D acc fake: 0.69], [G loss: 1.45]\n",
      "Epoch 0 Batch 214/1875 [D loss: 0.82, acc avg: 56.25%] [D acc real: 0.62 D acc fake: 0.50], [G loss: 1.52]\n",
      "Epoch 0 Batch 215/1875 [D loss: 0.92, acc avg: 46.88%] [D acc real: 0.50 D acc fake: 0.44], [G loss: 1.90]\n",
      "Epoch 0 Batch 216/1875 [D loss: 0.64, acc avg: 65.62%] [D acc real: 0.69 D acc fake: 0.62], [G loss: 2.08]\n",
      "Epoch 0 Batch 217/1875 [D loss: 0.81, acc avg: 56.25%] [D acc real: 0.50 D acc fake: 0.62], [G loss: 1.89]\n",
      "Epoch 0 Batch 218/1875 [D loss: 0.69, acc avg: 59.38%] [D acc real: 0.69 D acc fake: 0.50], [G loss: 1.90]\n",
      "Epoch 0 Batch 219/1875 [D loss: 0.78, acc avg: 50.00%] [D acc real: 0.56 D acc fake: 0.44], [G loss: 1.83]\n",
      "Epoch 0 Batch 220/1875 [D loss: 0.68, acc avg: 50.00%] [D acc real: 0.50 D acc fake: 0.50], [G loss: 1.46]\n",
      "Epoch 0 Batch 221/1875 [D loss: 0.72, acc avg: 59.38%] [D acc real: 0.69 D acc fake: 0.50], [G loss: 1.49]\n",
      "Epoch 0 Batch 222/1875 [D loss: 0.72, acc avg: 65.62%] [D acc real: 0.44 D acc fake: 0.88], [G loss: 1.18]\n",
      "Epoch 0 Batch 223/1875 [D loss: 0.64, acc avg: 59.38%] [D acc real: 0.75 D acc fake: 0.44], [G loss: 1.77]\n",
      "Epoch 0 Batch 224/1875 [D loss: 0.70, acc avg: 62.50%] [D acc real: 0.56 D acc fake: 0.69], [G loss: 1.20]\n",
      "Epoch 0 Batch 225/1875 [D loss: 1.08, acc avg: 37.50%] [D acc real: 0.44 D acc fake: 0.31], [G loss: 1.24]\n",
      "Epoch 0 Batch 226/1875 [D loss: 0.77, acc avg: 56.25%] [D acc real: 0.62 D acc fake: 0.50], [G loss: 1.95]\n",
      "Epoch 0 Batch 227/1875 [D loss: 0.75, acc avg: 53.12%] [D acc real: 0.50 D acc fake: 0.56], [G loss: 1.34]\n",
      "Epoch 0 Batch 228/1875 [D loss: 0.74, acc avg: 46.88%] [D acc real: 0.56 D acc fake: 0.38], [G loss: 2.10]\n",
      "Epoch 0 Batch 229/1875 [D loss: 0.86, acc avg: 50.00%] [D acc real: 0.44 D acc fake: 0.56], [G loss: 1.59]\n",
      "Epoch 0 Batch 230/1875 [D loss: 0.42, acc avg: 78.12%] [D acc real: 0.75 D acc fake: 0.81], [G loss: 1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 231/1875 [D loss: 0.81, acc avg: 50.00%] [D acc real: 0.69 D acc fake: 0.31], [G loss: 1.59]\n",
      "Epoch 0 Batch 232/1875 [D loss: 0.55, acc avg: 81.25%] [D acc real: 0.75 D acc fake: 0.88], [G loss: 1.57]\n",
      "Epoch 0 Batch 233/1875 [D loss: 0.84, acc avg: 53.12%] [D acc real: 0.56 D acc fake: 0.50], [G loss: 1.83]\n",
      "Epoch 0 Batch 234/1875 [D loss: 0.59, acc avg: 62.50%] [D acc real: 0.50 D acc fake: 0.75], [G loss: 1.76]\n",
      "Epoch 0 Batch 235/1875 [D loss: 0.86, acc avg: 56.25%] [D acc real: 0.69 D acc fake: 0.44], [G loss: 1.85]\n",
      "Epoch 0 Batch 236/1875 [D loss: 0.44, acc avg: 84.38%] [D acc real: 0.88 D acc fake: 0.81], [G loss: 2.18]\n",
      "Epoch 0 Batch 237/1875 [D loss: 0.60, acc avg: 71.88%] [D acc real: 0.62 D acc fake: 0.81], [G loss: 1.33]\n",
      "Epoch 0 Batch 238/1875 [D loss: 0.87, acc avg: 50.00%] [D acc real: 0.50 D acc fake: 0.50], [G loss: 1.24]\n",
      "Epoch 0 Batch 239/1875 [D loss: 0.72, acc avg: 59.38%] [D acc real: 0.62 D acc fake: 0.56], [G loss: 1.45]\n",
      "Epoch 0 Batch 240/1875 [D loss: 0.55, acc avg: 65.62%] [D acc real: 0.81 D acc fake: 0.50], [G loss: 1.78]\n",
      "Epoch 0 Batch 241/1875 [D loss: 0.74, acc avg: 56.25%] [D acc real: 0.44 D acc fake: 0.69], [G loss: 1.17]\n",
      "Epoch 0 Batch 242/1875 [D loss: 0.84, acc avg: 56.25%] [D acc real: 0.75 D acc fake: 0.38], [G loss: 1.20]\n",
      "Epoch 0 Batch 243/1875 [D loss: 0.81, acc avg: 56.25%] [D acc real: 0.62 D acc fake: 0.50], [G loss: 1.63]\n",
      "Epoch 0 Batch 244/1875 [D loss: 0.76, acc avg: 56.25%] [D acc real: 0.56 D acc fake: 0.56], [G loss: 2.39]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a8d33f829b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = load_data()\n",
    "\n",
    "epochs=2        # you need about 40 epochs to get good images\n",
    "batch_size=32\n",
    "save_interval=1\n",
    "\n",
    "num_examples = X_train.shape[0]\n",
    "num_batches = int(num_examples / float(batch_size))\n",
    "print('Number of examples: ', num_examples)\n",
    "print('Number of Batches: ', num_batches)\n",
    "print('Number of epochs: ', epochs)\n",
    "\n",
    "half_batch = int(batch_size / 2)\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    for batch in range(num_batches):\n",
    "\n",
    "            # noise images for the batch\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        fake_images = generator.predict(noise)\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "\n",
    "            # real images for batch\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_images = X_train[idx]\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "            # Train the generator\n",
    "        g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "            # Plot the progress\n",
    "        print(\"Epoch %d Batch %d/%d [D loss: %.2f, acc avg: %.2f%%] [D acc real: %.2f D acc fake: %.2f], [G loss: %.2f]\" %\n",
    "                  (epoch,batch, num_batches, d_loss[0], 100 * d_loss[1], d_loss_real[1], d_loss_fake[1],g_loss))\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            save_imgs(generator, epoch, batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Cool Use Case for GANs\n",
    "\n",
    "\n",
    "\n",
    "From: https://towardsdatascience.com/generative-adversarial-networks-gans-a-beginners-guide-5b38eceece24\n",
    "\n",
    "\"In addition to generating beautiful pictures, an approach for semi-supervised learning with GANs has been developed that involves the discriminator producing an additional output indicating the label of the input. This approach enables cutting edge results on datasets with very few labeled examples. On MNIST, for example, 99.1% accuracy has been achieved with only 10 labeled examples per class with a fully connected neural network — a result that’s very close to the best known results with fully supervised approaches using all 60,000 labeled examples. This is extremely promising because labeled examples can be quite expensive to obtain in practice.\"\n",
    "\n",
    "(NOTE: The above medium article is no longer available.   Here is a similar article, but without the above quote:\n",
    "https://skymind.ai/wiki/generative-adversarial-network-gan )\n",
    "\n",
    "Here is an example of how this idea (of semi-supervised learning) is implemented in practice:\n",
    "https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit - 5 pts!\n",
    "This could be a bit difficult:\n",
    "* Run the python version of the above code.  You can find a .py file and a .sh file in this directory to start with.  The shell script can be used with **qsub** to submit the script to a CPU (there is also one for the GPU but I did not find much evidence of a speedup and given the time to actually start a GPU job the CPU one may be your best bet).\n",
    "* Modify the python code to save a version of the generator model.  You will need to train the GAN for at least 15 epochs to generate realistic images.   This will take about 30 minutes using a CPU.  It would make sense to use something like the file naming procedure used for the images to name your model (meaning you could save the model every epoch, and label the filename using the current epoch number).\n",
    "* Use a version of the MNIST classsifier that you made in Assignment 10.  You should have a saved model of the trained version of that classifier - copy it here.  If you can't find it, a version that I made can be found in the scratch area:\n",
    "\n",
    "/fs/scratch/PAS1585/GAN/fully_trained_model_cnn.h5\n",
    "\n",
    "* Load both of these models (the CNN MNIST classifier and the generator model).  Then do the follwoing:  \n",
    "    * Use the GAN generator model to generate 10000 fake digits\n",
    "    * Feed them into your MNIST classifier and make two plots: \n",
    "        * A histogram of which digit your fakes were classified as.  \n",
    "        * The probability of your chosen digits. (You could also see if this varies by digit but that could be overkill!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Generator -- \n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_17 (Reshape)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_33 (UpSampling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_34 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,705\n",
      "Trainable params: 856,065\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#load generator model\n",
    "generator= build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "n_generator = 15\n",
    "generator.load_weights('generator_'+str(n_generator)+'.h5')\n",
    "\n",
    "#load CNN MNIST classifier\n",
    "cnn_network = models.Sequential()\n",
    "cnn_network.add(layers.Conv2D(30,(5,5),activation='relu',input_shape=(28,28,1)))\n",
    "cnn_network.add(layers.MaxPooling2D((2,2)))\n",
    "cnn_network.add(layers.Conv2D(25,(5,5),activation='relu'))\n",
    "cnn_network.add(layers.MaxPooling2D((2,2)))\n",
    "cnn_network.add(layers.Flatten())\n",
    "cnn_network.add(layers.Dense(64,activation='relu'))\n",
    "cnn_network.add(layers.Dense(10,activation='softmax'))\n",
    "cnn_network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "cnn_network.load_weights('/fs/scratch/PAS1585/GAN/fully_trained_model_cnn.h5')\n",
    "\n",
    "#use GAN generator to generate 10000 fake images\n",
    "noise = np.random.normal(0, 1, (1000, 100))\n",
    "fake_images = generator.predict(noise)\n",
    "\n",
    "from collections import defaultdict\n",
    "def autovivify(levels=1, final=dict):\n",
    "    return (defaultdict(final) if levels < 2 else\n",
    "            defaultdict(lambda: autovivify(levels-1, final)))\n",
    "\n",
    "#use MNIST-trained CNN to evaluate the generated images\n",
    "predictions = cnn_network.predict(fake_images)\n",
    "\n",
    "#plot CNN predicted classes of generated images\n",
    "probs = np.max(predictions, axis = 1)\n",
    "classes = np.argmax(predictions, axis = 1)\n",
    "\n",
    "num_bins = 10\n",
    "plt.hist(classes, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 0 7 4 6 3 7 7 6 6 7 5 5 6 7 7 0 5 5 3 2 7 2 9 6 1 5 1 5 7 3 5 0 2 3 2\n",
      " 0 3 0 3 8 2 3 0 2 0 9 5 0 3 5 5 4 2 6 3 4 7 9 3 3 3 7 2 6 3 7 0 0 9 0 7 1\n",
      " 1 1 0 3 5 5 1 3 2 5 2 4 4 3 1 1 5 3 3 7 3 9 7 3 7 9 9 3 3 8 5 3 0 6 7 4 7\n",
      " 8 1 3 6 7 2 7 9 5 5 7 6 9 3 4 7 0 2 3 6 2 3 8 9 7 7 3 9 6 7 4 9 0 3 5 1 3\n",
      " 2 7 4 7 3 2 5 0 7 4 3 5 1 7 2 1 3 5 4 6 7 1 1 0 2 2 4 3 2 5 3 1 3 8 3 7 6\n",
      " 7 7 7 2 0 1 4 1 3 9 0 1 3 0 3 9 2 3 9 2 2 3 8 5 0 5 3 5 8 3 3 6 3 3 1 5 2\n",
      " 9 3 3 3 6 6 3 9 0 9 1 3 7 5 2 1 3 8 9 3 2 6 2 7 3 3 0 6 2 1 1 9 6 4 5 5 0\n",
      " 3 2 9 9 2 2 5 2 9 3 8 7 3 1 7 9 4 2 3 3 8 5 9 5 2 2 3 4 1 6 6 9 3 7 9 6 4\n",
      " 7 7 5 4 3 1 8 5 1 3 5 0 3 2 9 1 5 5 9 4 5 6 3 0 3 3 3 9 3 2 3 3 3 8 2 1 3\n",
      " 1 5 7 5 9 4 2 7 3 1 1 4 3 3 8 9 3 6 3 6 8 9 4 4 9 9 1 1 7 6 2 0 2 9 1 9 7\n",
      " 9 5 1 5 4 3 9 3 9 7 5 5 3 5 9 9 7 7 5 7 3 5 3 4 2 3 3 3 1 3 4 2 9 3 7 3 3\n",
      " 3 4 3 0 6 2 9 5 3 2 5 9 6 7 4 7 9 8 7 3 2 3 6 7 5 5 2 3 7 3 7 2 1 2 0 6 1\n",
      " 8 7 3 3 7 6 0 5 1 3 3 5 3 1 5 2 7 6 5 7 8 3 0 2 2 3 5 8 1 4 2 5 3 2 0 1 1\n",
      " 8 6 5 5 2 2 7 0 5 0 3 9 9 6 7 9 6 1 1 7 4 8 7 6 3 1 0 3 3 7 8 2 9 3 3 2 3\n",
      " 9 7 2 9 3 4 4 3 8 3 2 3 4 3 9 0 3 2 3 7 0 1 1 3 3 4 3 3 9 3 2 4 2 4 2 3 4\n",
      " 0 6 3 5 5 3 2 9 3 3 9 0 7 7 1 3 4 0 6 2 1 3 7 0 1 8 3 3 9 7 8 5 0 5 6 3 3\n",
      " 7 9 5 9 4 1 6 8 3 3 1 5 7 8 3 0 1 5 2 5 3 1 0 4 3 4 0 1 3 3 3 3 6 5 5 6 6\n",
      " 9 2 4 3 4 3 9 5 3 3 1 2 1 7 7 2 6 4 2 2 3 3 5 1 3 6 8 2 6 3 5 8 3 8 0 9 0\n",
      " 9 5 2 2 5 8 3 4 0 7 6 3 8 1 9 0 3 2 7 6 3 3 2 1 9 6 6 5 3 3 0 5 0 4 7 6 5\n",
      " 5 3 2 3 5 6 1 6 3 8 5 2 3 3 3 3 3 7 7 4 6 4 4 5 7 2 3 1 7 3 8 6 4 3 0 3 1\n",
      " 9 3 3 7 8 7 1 3 8 3 2 0 7 0 3 9 1 6 6 1 3 5 8 7 5 7 5 3 6 8 5 2 3 8 7 8 5\n",
      " 7 1 7 4 9 7 7 2 0 6 6 4 1 3 3 5 2 4 2 7 2 1 3 2 1 3 5 3 3 4 3 9 3 8 7 3 9\n",
      " 4 2 5 5 3 3 4 2 9 3 9 0 3 1 8 2 2 3 9 1 3 7 4 0 2 2 5 2 0 5 7 2 2 7 4 3 2\n",
      " 5 8 4 1 3 1 3 4 6 4 8 5 7 9 7 3 4 3 1 2 3 0 0 2 2 3 8 3 3 5 3 7 3 3 6 7 3\n",
      " 2 3 3 2 2 6 4 3 0 0 2 5 0 4 2 5 3 9 9 7 7 3 1 9 1 0 1 3 3 3 2 5 9 7 2 2 3\n",
      " 1 8 2 4 3 1 3 5 3 5 3 5 9 1 1 4 7 3 2 7 4 8 5 7 7 3 3 3 3 3 2 6 7 0 3 7 3\n",
      " 1 4 3 4 0 4 3 1 6 8 2 2 8 7 5 3 4 5 7 5 1 5 2 0 2 1 3 0 3 0 2 4 1 6 6 3 7\n",
      " 6]\n"
     ]
    }
   ],
   "source": [
    "num_bins = 10\n",
    "plt.clf()\n",
    "plt.hist(classes, num_bins)\n",
    "plt.show()\n",
    "plt.savefig('classes.png')\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(probs))\n",
    "plt.clf()\n",
    "plt.hist(probs, num_bins)\n",
    "plt.show()\n",
    "plt.savefig('probs.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
