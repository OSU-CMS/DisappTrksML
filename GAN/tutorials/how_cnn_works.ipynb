{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Convnets Work\n",
    "Let's get our data and re-build the convnet from cnn_intro.ipynb\n",
    "\n",
    "First the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info (60000, 28, 28) (60000,)\n",
      "Test info (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "short = False\n",
    "if short:\n",
    "    train_images = train_images[:7000,:]\n",
    "    train_labels = train_labels[:7000]\n",
    "    test_images = test_images[:3000,:]\n",
    "    test_labels = test_labels[:3000]\n",
    "#\n",
    "print(\"Train info\",train_images.shape, train_labels.shape)\n",
    "print(\"Test info\",test_images.shape, test_labels.shape)\n",
    "train_images = train_images.reshape((train_images.shape[0],28*28))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((test_images.shape[0],28*28))\n",
    "test_images = test_images.astype('float32')/255\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_cat = to_categorical(train_labels)\n",
    "test_labels_cat = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 30)        780       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 25)          18775     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 25)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                25664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 45,869\n",
      "Trainable params: 45,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "#\n",
    "# Make sure the shape of the input is correct (the last \",1\" is the number of \"channels\"=1 for grayscale)\n",
    "train_images = train_images.reshape((train_images.shape[0],28,28,1))\n",
    "test_images = test_images.reshape((test_images.shape[0],28,28,1))\n",
    "#\n",
    "cnn_network = models.Sequential()\n",
    "#\n",
    "# First convolutional layer\n",
    "cnn_network.add(layers.Conv2D(30,(5,5),activation='relu',input_shape=(28,28,1)))\n",
    "# Pool\n",
    "cnn_network.add(layers.MaxPooling2D((2,2)))\n",
    "#\n",
    "# Second convolutional layer\n",
    "cnn_network.add(layers.Conv2D(25,(5,5),activation='relu'))\n",
    "# Pool\n",
    "cnn_network.add(layers.MaxPooling2D((2,2)))\n",
    "#\n",
    "# Connect to a dense output layer - just like an FCN\n",
    "cnn_network.add(layers.Flatten())\n",
    "cnn_network.add(layers.Dense(64,activation='relu'))\n",
    "cnn_network.add(layers.Dense(10,activation='softmax'))\n",
    "#\n",
    "# Compile\n",
    "cnn_network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "# Fit/save/print summary\n",
    "#history = cnn_network.fit(train_images,train_labels_cat,epochs=5,batch_size=256,validation_data=(test_images,test_labels_cat))\n",
    "#cnn_network.save('fully_trained_model_cnn.h5')\n",
    "#\n",
    "# Instead of re-training, just load the network from disk!\n",
    "cnn_network.save('fully_trained_model_cnn.h5')\n",
    "\n",
    "print(cnn_network.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the CNN Works\n",
    "Much of the discussion here is based on this [article](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53).\n",
    "\n",
    "Here is a picture of the above network:\n",
    "![alt text](files/convet_graphic_mnist.jpeg \"Title\")\n",
    "\n",
    "Let's walk through the network from left to right:\n",
    "1.  The input image is 28x28x1.  The 28x28 are the horizontal and vertical count of pixels in the image.   The \"x1\" reminds us that typical images have 3 color channels, while our MNIST images are **grayscale** which have only 1 channel.\n",
    "2.  The first **convolution layer** uses a 5x5 kernel.   Jump ahead to the next section to see how the kernel works.  In the image, you see that the output of the convolution layer is a tensor of size (24 x 24 x n1), where **n1** refers to the number of filters in the convolution layer.  In our case, n1 is equal to 30 (it could be any number).   You should think of this output as n1=30 images, each of size 24x24.   But the input to the convolutional layer is 28x28!   Why did we lose 4 pixels in each direction?   The next section on kernel application explains this!\n",
    "    * The number of parameters of the first layer is given by this: (5x5) parameters for each filter, times 30 filters (or kernels), plus a bias unit for each filter: (5x5x30) + 30 = 780\n",
    "    * The purpose of the convolutional layer is to find common features across the image.\n",
    "    * The output of each filter (when applied to a given 5x5 section of the input to that layer) is passed through the **activation** function for that layer.  In CNNs the activation is often \"relu\", which stands for rectified linear unit.\n",
    "    * The output of the convolutional layer is a tensor of size (24x24x30), where 24x24 is the image size, and 30 is the number of filters.   We can think of this as a 24x24 image with 30 channels.\n",
    "3.  Next there is a **max pooling** layer, which uses a 2x2 kernel.   This simply passes a 2x2 kernel across the output of the previous layer, and it chooses the maximum of the 4 pixels it sees.   This results in the image being downscaled.  In our case, each 24x24 \"image\" becomes a \"12x12\" image.   There are other pooling algorithms which are possible, such as **average pooling**, but max pooling has been found to be much more effective.\n",
    "    * The are no parameters associated with the pooling layer.\n",
    "    * The output of this max pooling layer is a tensor of shape (12x12x30).   To help understand this, imagine we put a **single** 28x28 image into our network.  The output of this max pooling layer will be a 12x12 \"image\", with 30 channels!\n",
    "4. The second convolutional layer also uses a \"5x5\" kernel, and we have chosen (arbitrarily) to have 25 filters (or kernels).   But what is hidden in the above is that each kernel **must** also have a 3rd dimension, equal to that of the output channels of the previous layer (in this case 30).   So we actually have a 5x5x30 kernel.  This single 5x5x30 kernel is convolved across the 12x12x30 image, producing an output which is 8x8x1.  Note how the output image again loses 4 pixels in both height and width.\n",
    "    * In our second convolutional layer, we have 25 different filters, so the full output of the 2nd convolutional layer is: 8x8x25 (or an 8x8 image, again with 25 channels).\n",
    "    * Since this second convolutional layer is acting on a downscaled image, it is sensitive to larger features across the image (while the earlier convolutional layer was sensitive to smaller features in the image).\n",
    "    * The number of parameters of this layer can be calculated as such: (5x5)x30 = 750 parameters for each kernel, times 25 kernels which gives us 18750 paremeters, plus a bias unit for each of the 25 filters, which yields 18775 parameters.\n",
    "    * An excellent discussion of how this works can be found [here](https://www.youtube.com/watch?v=KTB_OFoAQcc&index=6&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF).\n",
    "5.  Again there is a max pooling layer, which serves to reduce the dimensions from (8x8x25) to (4x4x25).\n",
    "6.  Next we have a **flattening** layer, which simply converts the output of the previous layer from a tensor to an unwrapped vector of length 4x4x25=400.   There are no parameters associated with this operation.\n",
    "7.  Next we have a fully connected (hidden) layer with 64 nodes (the 64 is arbitrary - it could be 100 or 200 or 50). \n",
    "    * The number of parameters associated with this layer is (400x64) weights plus another 64 bias units for a total of 25664 parameters.\n",
    "    * The activation unit used is again **relu**.\n",
    "8. Finally we have a fully connected softmax layer, with 10 outputs, one for each digit.\n",
    "    * The number of parameters associated with this layer is (64x10) weights plus another 10 bias units for a total of 650 parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolution Kernel (Or Filter)\n",
    "The picture below is a simple graphic showing a 3x3 kernel (the yellow moving square) applied to a (5x5x1) image (the stationary green square on the left).   All of the pixel values in the image are 1.0, as are the numbers in the kernel, to simplify the math.   Some things to note:\n",
    "* The output of the kernel is simply obtained by multiplying the pixel value by the kernel, and adding this up for each pixel:kernel combination.\n",
    "* The \"stride\" of the kernel is (1,1) by default for keras Conv2D layers, which simply means that the kernel moves over by 1 pixel after each operation, and then down by 1 pixel when it gets to the end of each row.  The movement of the kernel across the image is the **convolution** operation.\n",
    "* Note that the output image is 3x3 and **not** 5x5.   This is because the 3x3 kernel runs out of pixels when it gets 2 pixels from the end of each row, as well as the end of each column.   So applying a 3x3 kernel to an (n by m) image, results in an output image of size (n-2) by (m-2).  Or more generally, (n-(p-1)) by (m-(p-1)), where (p by p) is the size the of kernel.\n",
    "    \n",
    "![alt text](files/anim_covnet.gif \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "The graphic below is an example of a 3x3 max pooling kernel applied to a 5x5 image.   Note that it simply outputs the maximum pixel value found in the region covered by the kernel.\n",
    "* The **strides** in this graphic are (1,1), which means that the kernel moves over by 1 pixel as it moves from left to right, and then down by 1 pixel as it moves from top to bottom.\n",
    "* If you look at our MaxPooling2D layer, we simply give the size of the kernel, which is (2,2).  By default, keras assumes the strides for MaxPooling2D layers are the **same** as the pooling kernel size (for pooling layers), so in our case the kernel would move over by 2 pixels, and down by 2 pixels.   This only works if there are an even number of pixels in the image that the max pooling layer processes (assuming no padding - see documentation).   The effect of this for our images is that max pooling **downscales** the images by a factor of 2.\n",
    "![alt text](files/pooling.gif \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
